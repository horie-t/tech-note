---
title: "SageMakerã§JParaCrawlã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹"
emoji: "ğŸƒ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["æ©Ÿæ¢°å­¦ç¿’", "æ©Ÿæ¢°ç¿»è¨³"]
published: true
---

Webä¸Šã§ã‚ˆãè¦‹ã‹ã‘ã‚‹æ©Ÿæ¢°ç¿»è¨³ã®ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ãŸä¾‹ã¯ã€[äº¬éƒ½ãƒ•ãƒªãƒ¼ç¿»è¨³ã‚¿ã‚¹ã‚¯ (KFTT)](http://www.phontron.com/kftt/index-ja.html)ã®å¯¾è¨³ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ãŸã‚‚ã®ãŒå¤šã„ã§ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã¯äº¬éƒ½é–¢é€£ã«ç‰¹åŒ–ã—ãŸã‚³ãƒ¼ãƒ‘ã‚¹ã§æ–‡æ•°ã‚‚44ä¸‡æ–‡ã¨å°‘ãªã„ã‚‚ã®ã§ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’Google Colabã®å¤ã„GPUã‚’ä½¿ã£ã¦çŸ­æ™‚é–“ã ã‘å­¦ç¿’ã•ã›ã¦ã¿ã¦ã€è¾›ã†ã˜ã¦ç¿»è¨³ã§ãã¦ã„ã‚‹ã‚ˆã†ãªçµæœã«ãªã‚‹ã‚‚ã®ãŒå¤šã„ã§ã™ã€‚

ã‚‚ã£ã¨æœ¬æ ¼çš„ãªã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ã€å¼·åŠ›ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å­¦ç¿’ã—ã¦ã¿ãŸã‚‰ã©ã†ãªã‚‹ã‹ã‚’ã€ä»Šå›è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚

ã¾ãšã€ã‚³ãƒ¼ãƒ‘ã‚¹ã¯[JParaCrawl](https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚JParaCrawlã¨ã¯NTTã«ã‚ˆã£ã¦ä½œæˆã•ã‚ŒãŸæœ€å¤§è¦æ¨¡(2000ä¸‡æ–‡ä»¥ä¸Š)ã®æ—¥è‹±å¯¾è¨³ã‚³ãƒ¼ãƒ‘ã‚¹ã§ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ã£ã¦ã€SageMakerã§`ml.g4dn.12xlarge`ã¨ã„ã†GPUãŒ4ã¤ã‚‚ã‚ã‚‹ãƒã‚·ãƒ³ã§å­¦ç¿’ã•ã›ã¦ã¿ã¾ã—ãŸã€‚

## å‰æº–å‚™

[ã“ã¡ã‚‰ã®è¨˜äº‹](https://zenn.dev/thorie/articles/548nl_sagemaker_serverless#%E3%83%A2%E3%83%87%E3%83%AB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AEs3%E3%83%90%E3%82%B1%E3%83%83%E3%83%88%E3%81%B8%E3%81%AE%E8%AA%AD%E3%81%BF%E5%8F%96%E3%82%8A%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9%E3%81%AErole%E3%82%92%E4%BD%9C%E6%88%90)ã«ã‚ã‚‹ã‚ˆã†ã«ã€AWSä¸Šã§IAM Roleã‚’ä½œæˆã—ã¦ãŠãã¾ã™ã€‚

## ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

[JParaCrawl](https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/)ã®ã‚µã‚¤ãƒˆã‹ã‚‰ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
!mkdir data
%cd data
```

```python
!wget https://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz
```

## ã‚³ãƒ¼ãƒ‘ã‚¹ã®å‰å‡¦ç†

OpenNMT-pyã§æ‰±ã„ã‚„ã™ã„ã‚ˆã†ã«ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¾ã™ã€‚
ã¾ãšã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹ã—ã¾ã™ã€‚


```python
!tar xvf en-ja.tar.gz
%cd en-ja/
```

ãƒ‡ãƒ¼ã‚¿ã®ä¸­èº«ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ã‚¿ãƒ–åŒºåˆ‡ã‚Šã§5ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã€4ã€5ç•ªç›®ã«ãã‚Œãã‚Œè‹±æ–‡ã€å’Œæ–‡ãŒã‚ã‚Šã¾ã—ãŸã€‚

æ–‡æ•°ã¯25,740,835ã§ã—ãŸã€‚


```python
!head en-ja.bicleaner05.txt
```

    0001vip.cocolog-nifty.com	0001vip.cocolog-nifty.com	0.535	And everyone will not care that it is not you.	é¼»ãƒ»å£ã®ã¨ã“ã‚ã¯ã‚ã‚‰ã‹ã˜ã‚å°‘ã—åˆ‡ã£ã¦ãŠãã¨ã„ã„ã§ã™ã­ã€‚
    0001vip.cocolog-nifty.com	0001vip.cocolog-nifty.com	0.557	And everyone will not care that it is not you.	ã‚¢ãƒ‰ãƒ¬ã‚¹ç½®ã„ã¨ãã®ã§ã€æ¶ˆã•ã‚Œãªã„ã†ã¡ã«ãƒ¡ãƒ¼ãƒ«ãã‚ŒãŸã‚‰å¬‰ã—ã„ã§ã™ã€‚
    000-lhr.web.wox.cc	000-lhr.web.wox.cc	0.743	Sponsored link This advertisement is displayed when there is no update for a certain period of time.	ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ ã“ã®åºƒå‘Šã¯ä¸€å®šæœŸé–“æ›´æ–°ãŒãªã„å ´åˆã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    000-lhr.web.wox.cc	000-lhr.web.wox.cc	0.750	Also, it will always be hidden when becoming a premium user .	ã¾ãŸã€ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ ã«ãªã‚‹ã¨å¸¸ã«éè¡¨ç¤ºã«ãªã‚Šã¾ã™ã€‚
    000-lhr.web.wox.cc	000-lhr.web.wox.cc	0.751	It will return to non-display when content update is done.	ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ›´æ–°ãŒè¡Œã‚ã‚Œã‚‹ã¨éè¡¨ç¤ºã«æˆ»ã‚Šã¾ã™ã€‚
    000-lhr.web.wox.cc	kapuri21.web.wox.cc	0.743	Sponsored link This advertisement is displayed when there is no update for a certain period of time.	ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ ã“ã®åºƒå‘Šã¯ä¸€å®šæœŸé–“æ›´æ–°ãŒãªã„å ´åˆã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    000-lhr.web.wox.cc	kapuri21.web.wox.cc	0.750	Also, it will always be hidden when becoming a premium user .	ã¾ãŸã€ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ ã«ãªã‚‹ã¨å¸¸ã«éè¡¨ç¤ºã«ãªã‚Šã¾ã™ã€‚
    000-lhr.web.wox.cc	kapuri21.web.wox.cc	0.751	It will return to non-display when content update is done.	ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ›´æ–°ãŒè¡Œã‚ã‚Œã‚‹ã¨éè¡¨ç¤ºã«æˆ»ã‚Šã¾ã™ã€‚
    0017.org	0017.org	0.500	Itâ€™s like you can enrich it and save money as a result. Iâ€™ve watched a lot of videos, mainly on Youtube, of people who say theyâ€™re minimalists, and I agree.	Youtubeã‚’ä¸­å¿ƒã«ãƒŸãƒ‹ãƒãƒªã‚¹ãƒˆã¨è¨€ã£ã¦ã„ã‚‹æ–¹ã®å‹•ç”»ã‚’ãŸãã•ã‚“ã¿ã¾ã—ãŸãŒã€ç´å¾—ã„ãã‚‚ã®ã®ã‚‚å¤šãã€ä¸æ³ã¨å°‘å­åŒ–ã‚’å‘¼ã°ã‚Œã‚‹â€ä»Šã®â€æ—¥æœ¬ã«ã¯åˆã£ã¦ã„ã‚‹è€ƒãˆæ–¹ãªã®ã‹ãªã€ã¨æ„Ÿã˜ã¦ã„ã¾ã™ã€‚
    0017.org	0017.org	0.502	Go to the original video hierarchy of the conversion source, copy and paste the following is fine. ffmpeg -i sample.mp4 -strict -2 video.webm summary Iâ€™ve been using the upload and embed method to Youtube to set up videos on the web.	ffmpeg -i sample.mp4 -strict -2 video.webm ã¾ã¨ã‚ Webä¸Šã§å‹•ç”»ã‚’è¨­ç½®ã™ã‚‹ã¨ãã¯ã€Youtubeã«ã‚¢ãƒƒãƒ—ã—ã¦åŸ‹ã‚è¾¼ã‚€æ–¹æ³•ã°ã‹ã‚Šä½¿ã£ã¦ã„ã¾ã—ãŸãŒã€ã“ã‚Œã§è¤‡æ•°ã®å‹•ç”»å½¢å¼ã‚’ä½œã‚‹ã“ã¨ãŒã§ãã‚‹ã®ã§ã€è‡ªåˆ†ã®ã‚µãƒ¼ãƒã«è¨­å®šã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã«ãªã‚Šã¾ã™ã­ã€‚



```python
!wc -l en-ja.bicleaner05.txt
```

    25740835 en-ja.bicleaner05.txt


è‹±æ–‡ã¨å’Œæ–‡ã ã‘ã‚’å–ã‚Šå‡ºã—ã¾ã™ã€‚


```python
!cat en-ja.bicleaner05.txt | cut -f4 > en.txt
!cat en-ja.bicleaner05.txt | cut -f5 > ja.txt
```

## æ–‡ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

æ¬¡ã«å­¦ç¿’ç”¨ã«æ–‡ç« ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚

ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã¯[MeCab](https://taku910.github.io/mecab/)ç­‰ãŒç”¨ã„ã‚‰ã‚Œã‚‹ã“ã¨ã‚‚å¤šã„ã®ã§ã™ãŒã€ã“ã“ã§ã¯ã€[SentencePiece](https://github.com/google/sentencepiece)ã‚’ä½¿ã£ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã¿ã¾ã™ã€‚

ã¾ãšã€Pythonã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚


```python
!pip install sentencepiece
```

å…¨éƒ¨ã®æ–‡ç« ã‹ã‚‰å˜èªåˆ†å‰²ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ç­†è€…ã®PCã®ãƒ¡ãƒ¢ãƒª(8GB)ã§ã¯è¶³ã‚Šãªã‹ã£ãŸã®ã§ã€ä¸€éƒ¨ã®æ–‡(100,000æ–‡)ã‹ã‚‰å­¦ç¿’ã—ã¾ã—ãŸã€‚

PCã®ç’°å¢ƒã«ã‚ˆã£ã¦ã¯æ–‡æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚ã‚ˆã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

ã¾ãšæœ€åˆã«ã€è‹±æ–‡ã€å’Œæ–‡ã®ä¸­èº«ã®ä¸€éƒ¨ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å–ã‚Šå‡ºã—ã¾ã™ã€‚


```python
!shuf -n 100000 en.txt -o vocab_train.en
!shuf -n 100000 ja.txt -o vocab_train.ja
```

å–ã‚Šå‡ºã—ãŸæ–‡ã§å­¦ç¿’ã—ã¾ã™ã€‚

character_coverageã¯ã€è‹±èªã®å ´åˆã¯å…¬å¼ã‚µã‚¤ãƒˆã§ä¾‹ç¤ºã•ã‚Œã¦ã„ã‚‹å€¤ã€æ—¥æœ¬èªã®å ´åˆã¯ä¸€èˆ¬çš„ã«ã‚ˆã„ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹å€¤ã§ã™ã€‚


```python
import sentencepiece as spm

spm.SentencePieceTrainer.Train(
   '--input=vocab_train.en --model_prefix=sentencepiece_en --vocab_size=32000 --character_coverage=0.98'
)
spm.SentencePieceTrainer.Train(
   '--input=vocab_train.ja --model_prefix=sentencepiece_ja --vocab_size=32000 --character_coverage=0.9995'
)
```

å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€è©¦ã—ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã¿ã¾ã™ã€‚

è‹±èªã®å ´åˆã¯ç©ºç™½ã§åŒºåˆ‡ã£ãŸã ã‘ã®ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€æ—¥æœ¬èªã®å ´åˆã¯æ–‡æ³•é€šã‚Šã§ã¯ãªãç‹¬è‡ªã«å­¦ç¿’ã—ã¦ã„ã‚‹ã‚ˆã†ã§ã™ã€‚


```python
sp_en = spm.SentencePieceProcessor("sentencepiece_en.model")
print(sp_en.encode("It will return to non-display when content update is done.", out_type=str))
```

    ['â–It', 'â–will', 'â–return', 'â–to', 'â–non', '-', 'display', 'â–when', 'â–content', 'â–update', 'â–is', 'â–done', '.']



```python
sp_ja = spm.SentencePieceProcessor("sentencepiece_ja.model")
print(' '.join(sp_ja.encode("ã‚¹ãƒãƒ³ã‚µãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ ã“ã®åºƒå‘Šã¯ä¸€å®šæœŸé–“æ›´æ–°ãŒãªã„å ´åˆã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚", out_type=str)))
```

    â– ã‚¹ãƒãƒ³ã‚µãƒ¼ ãƒ‰ ãƒªãƒ³ã‚¯ â–ã“ã® åºƒå‘Š ã¯ ä¸€å®š æœŸé–“ æ›´æ–° ãŒãªã„å ´åˆ ã«è¡¨ç¤ºã•ã‚Œã¾ã™ ã€‚


ã‚³ãƒ¼ãƒ‘ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã¨ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ã¦ã€ç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚


```python
with open("./en.txt") as in_f:
    with open("en_tokenized.txt", mode='w') as out_f:
        for line in in_f:
            out_f.write(' '.join(sp_en.encode(line, out_type=str)) + "\n")
```


```python
with open("./ja.txt") as in_f:
    with open("ja_tokenized.txt", mode='w') as out_f:
        for line in in_f:
            out_f.write(' '.join(sp_ja.encode(line, out_type=str)) + "\n")
```

ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ãŸã‚‰ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€éƒ¨(25730000æ–‡)ã‚’è¨“ç·´ç”¨ã€5000æ–‡ã‚’æ¤œè¨¼ç”¨ã€5000æ–‡ã‚’ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†ã‘ã¾ã™ã€‚

ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®é ­ã‹ã‚‰ä½•ä¸‡æ–‡ã¨è¨€ã†ã‚ˆã†ã«åˆ†ã‘ã‚‹ã¨ã€å–å¾—å…ƒã®ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒåã‚‹ã®ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

ã¾ãšæœ€åˆã«ã‚³ãƒ¼ãƒ‘ã‚¹ã®æ–‡æ•°ã¾ã§ã®æ•°å€¤ãŒãƒ©ãƒ³ãƒ€ãƒ ã«ãªã‚‰ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚


```python
!seq `wc -l en-ja.bicleaner05.txt | cut -f1 -d' '` | shuf -o line_nums.txt -
```


```python
è¨“ç·´ç”¨ã€æ¤œè¨¼ç”¨ã€ãƒ†ã‚¹ãƒˆç”¨ã®æ–‡ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã®è¡Œç•ªå·ãŒå…¥ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
```


```python
!head --lines=25730000 line_nums.txt | sort --numeric-sort > train_line_nums.txt
!head --lines=25735000 line_nums.txt | tail --lines=5000 | sort --numeric-sort > val_line_nums.txt
!head --lines=25740000 line_nums.txt | tail --lines=5000 | sort --numeric-sort > test_line_nums.txt
```

æ–‡ã®ãƒ•ã‚¡ã‚¤ãƒ«åã¨ã€è¡Œç•ªå·ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã€è¡Œç•ªå·ã®æ–‡ã‚’å–ã‚Šå‡ºã™é–¢æ•°ã‚’å®šç¾©ã—ã¦ã€è¨“ç·´ç”¨ã€æ¤œè¨¼ç”¨ã€ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚


```python
# input_file: å–ã‚Šå‡ºã—å…ƒãƒ•ã‚¡ã‚¤ãƒ«
# num_fileã®è¡Œç•ªå·ã®è¡Œã ã‘å–ã‚Šå‡ºã—ã¾ã™ã€‚
# output_file: å–ã‚Šå‡ºã—ãŸçµæœãƒ•ã‚¡ã‚¤ãƒ«
def extract_lines(input_file, num_file, output_file):
    text_line_num = 1
    with open(num_file) as line_f:
        line_num = int(line_f.readline())
        with open(input_file) as in_f:
            with open(output_file, mode='w') as out_f:
                for line in in_f:
                    if text_line_num == line_num:
                        out_f.write(line)
                        line_num = line_f.readline()
                        if line_num == '':
                            break
                        else:
                            line_num = int(line_num)
                    text_line_num += 1
```


```python
extract_lines("en_tokenized.txt", "train_line_nums.txt", "train.en")
extract_lines("ja_tokenized.txt", "train_line_nums.txt", "train.ja")
extract_lines("en_tokenized.txt", "val_line_nums.txt", "val.en")
extract_lines("ja_tokenized.txt", "val_line_nums.txt", "val.ja")
extract_lines("en_tokenized.txt", "test_line_nums.txt", "test.en")
extract_lines("ja_tokenized.txt", "test_line_nums.txt", "test.ja")
```

è©¦ã—ã«æ—¥æœ¬èªã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚


```python
!head train.ja
```

    â– é¼» ãƒ» å£ ã® ã¨ã“ã‚ ã¯ ã‚ã‚‰ã‹ã˜ã‚ å°‘ã— åˆ‡ ã£ã¦ ãŠ ã ã¨ ã„ã„ ã§ã™ã­ ã€‚
    â– ã‚¢ãƒ‰ãƒ¬ã‚¹ ç½® ã„ ã¨ ã ã®ã§ ã€ æ¶ˆ ã•ã‚Œãªã„ ã†ã¡ ã« ãƒ¡ãƒ¼ãƒ« ã ã‚ŒãŸ ã‚‰ å¬‰ã—ã„ ã§ã™ ã€‚
    â– ã‚¹ãƒãƒ³ã‚µãƒ¼ ãƒ‰ ãƒªãƒ³ã‚¯ â–ã“ã® åºƒå‘Š ã¯ ä¸€å®š æœŸé–“ æ›´æ–° ãŒãªã„å ´åˆ ã«è¡¨ç¤ºã•ã‚Œã¾ã™ ã€‚
    â–ã¾ãŸ ã€ â– ãƒ—ãƒ¬ãƒŸã‚¢ãƒ  ãƒ¦ãƒ¼ã‚¶ãƒ¼ â– ã«ãªã‚‹ã¨ å¸¸ã« éè¡¨ç¤º ã«ãªã‚Šã¾ã™ ã€‚
    â– ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ã® æ›´æ–° ãŒ è¡Œã‚ã‚Œã‚‹ ã¨ éè¡¨ç¤º ã«æˆ»ã‚Šã¾ã™ ã€‚
    â– ã‚¹ãƒãƒ³ã‚µãƒ¼ ãƒ‰ ãƒªãƒ³ã‚¯ â–ã“ã® åºƒå‘Š ã¯ ä¸€å®š æœŸé–“ æ›´æ–° ãŒãªã„å ´åˆ ã«è¡¨ç¤ºã•ã‚Œã¾ã™ ã€‚
    â–ã¾ãŸ ã€ â– ãƒ—ãƒ¬ãƒŸã‚¢ãƒ  ãƒ¦ãƒ¼ã‚¶ãƒ¼ â– ã«ãªã‚‹ã¨ å¸¸ã« éè¡¨ç¤º ã«ãªã‚Šã¾ã™ ã€‚
    â– ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ã® æ›´æ–° ãŒ è¡Œã‚ã‚Œã‚‹ ã¨ éè¡¨ç¤º ã«æˆ»ã‚Šã¾ã™ ã€‚
    â–You tu be ã‚’ä¸­å¿ƒã« ãƒŸãƒ‹ ãƒ ãƒªã‚¹ãƒˆ ã¨è¨€ã£ã¦ ã„ã‚‹ æ–¹ ã® å‹•ç”» ã‚’ ãŸãã•ã‚“ ã¿ ã¾ã—ãŸ ãŒ ã€ ç´ å¾— ã„ã ã‚‚ã®ã® ã‚‚ å¤šã ã€ ä¸ æ³ ã¨ å°‘ å­ åŒ– ã‚’ å‘¼ã°ã‚Œ ã‚‹ â€ ä»Š ã® â€ æ—¥æœ¬ ã«ã¯ åˆ ã£ã¦ã„ã‚‹ è€ƒãˆæ–¹ ãªã®ã‹ ãª ã€ ã¨ æ„Ÿã˜ ã¦ã„ã¾ã™ ã€‚
    â–f f mp e g â–- i â– sa mp le . mp 4 â–- str ic t â– -2 â– v ide o . web m â– ã¾ã¨ã‚ â–Web ä¸Šã§ å‹•ç”» ã‚’ è¨­ç½® ã™ã‚‹ã¨ã ã¯ ã€ Y out ub e ã« ã‚¢ãƒƒãƒ— ã—ã¦ åŸ‹ã‚ è¾¼ã‚€ æ–¹æ³• ã°ã‹ã‚Š ä½¿ ã£ã¦ã„ã¾ã—ãŸ ãŒ ã€ ã“ã‚Œ ã§ è¤‡æ•°ã® å‹•ç”» å½¢å¼ ã‚’ä½œã‚‹ ã“ã¨ãŒã§ãã‚‹ ã®ã§ ã€ è‡ªåˆ†ã® ã‚µãƒ¼ãƒ ã« è¨­å®š ã™ã‚‹ã“ã¨ã‚‚ å¯èƒ½ ã«ãªã‚Šã¾ã™ ã­ ã€‚

```python
%cd ../..
```

## OpenNMT-pyã§å­¦ç¿’

ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ãŒã§ããŸã®ã§ã€OpenNMT-pyã§å­¦ç¿’ã•ã›ã¾ã™ã€‚

ã¾ãšã¯OpenNMT-pyã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚


```python
!pip install OpenNMT-py
```

ä»¥ä¸‹ã®ã‚ˆã†ãªYamlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```yml:vocab_en_ja.yml
save_data: data/en-ja/jparacrawl
## Where the vocab(s) will be written
src_vocab: data/en-ja/jparacrawl.vocab.src
tgt_vocab: data/en-ja/jparacrawl.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: data/en-ja/train.en
        path_tgt: data/en-ja/train.ja
    valid:
        path_src: data/en-ja/val.en
        path_tgt: data/en-ja/val.ja

```

ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªãƒ¼(èªå½™)ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
!onmt_build_vocab -config src/bocab_en_ja.yml -n_sample 10000
```

å­¦ç¿’ã«æœ¬å½“ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã ã‘åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é›†ã‚ã¾ã™ã€‚

```bash
mkdir -p data_train/en_ja
cp data/en_ja/jparacrawl.vocab.* data_train/en_ja
cp data/en-ja/train.* data_train/en_ja
cp data/en-ja/val.* data_train/en_ja
```

ä»¥ä¸‹ã®ã‚ˆã†ãªå­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®yamlãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
```yml
save_data: /opt/ml/input/data/training/en-ja/jparacrawl
## Where the vocab(s) will be written
src_vocab: /opt/ml/input/data/training/en-ja/jparacrawl.vocab.src
tgt_vocab: /opt/ml/input/data/training/en-ja/jparacrawl.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: /opt/ml/input/data/training/en-ja/train.en
        path_tgt: /opt/ml/input/data/training/en-ja/train.ja
    valid:
        path_src: /opt/ml/input/data/training/en-ja/val.en
        path_tgt: /opt/ml/input/data/training/en-ja/val.ja

#data: /opt/ml/model/dataset.en_ja
save_model: /opt/ml/model/model.en-ja
save_checkpoint_steps: 10000
keep_checkpoint: 10
seed: 3435
train_steps: 40000
valid_steps: 10000
warmup_steps: 8000
report_every: 100

decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
rnn_size: 512
layers: 6
transformer_ff: 2048
heads: 8

accum_count: 2
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 2.0
max_grad_norm: 0.0

batch_size: 4096
batch_type: tokens
normalization: tokens
dropout: 0.1
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

world_size: 4
gpu_ranks:
- 0
- 1
- 2
- 3
```

ä»¥ä¸‹ã®ã‚ˆã†ãªå­¦ç¿’ã‚³ãƒãƒ³ãƒ‰ã‚’å‘¼ã³å‡ºã™ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
#!/usr/bin/env bash

onmt_train -config train_en_ja.yml
```

ä»¥ä¸‹ã®ã‚ˆã†ãªDockerãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚
```docker
FROM nvidia/cuda:11.7.1-runtime-ubuntu20.04

RUN apt-get -y update
RUN apt-get -y install python3 pip

RUN pip --no-cache-dir install OpenNMT-py

ENV PYTHONUNBUFFERED=TRUE
ENV PYTHONDONTWRITEBYTECODE=TRUE
ENV PATH="/opt/program:${PATH}"

COPY src /opt/program
WORKDIR /opt/program
```

ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆã—ã€Amazon Elastic Container Registry(ECR)ã«ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’pushã—ã¾ã™ã€‚

```bash:build_and_push.sh
#!/usr/bin/env bash

# This script shows how to build the Docker image and push it to ECR to be ready for use
# by SageMaker.

# The argument to this script is the image name. This will be used as the image on the local
# machine and combined with the account and region to form the repository name for ECR.
image=$1

if [ "$image" == "" ]
then
    echo "Usage: $0 <image-name>"
    exit 1
fi

chmod +x src/train
chmod +x src/serve

# Get the account number associated with the current IAM credentials
account=$(aws sts get-caller-identity --query Account --output text)

if [ $? -ne 0 ]
then
    exit 255
fi


# Get the region defined in the current configuration (default to us-west-2 if none defined)
region=$(aws configure get region)
region=${region:-us-west-2}


fullname="${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest"

# If the repository doesn't exist in ECR, create it.

aws ecr describe-repositories --repository-names "${image}" > /dev/null 2>&1

if [ $? -ne 0 ]
then
    aws ecr create-repository --repository-name "${image}" > /dev/null
fi

# Get the login command from ECR and execute it directly
aws ecr get-login-password --region "${region}" | docker login --username AWS --password-stdin "${account}".dkr.ecr."${region}".amazonaws.com

# Build the docker image locally with the image name and then push it to ECR
# with the full name.

docker build  -t ${image} .
docker tag ${image} ${fullname}

docker push ${fullname}
```

```python
!./build_and_push.sh jparacrawl-train
```

ä½œæˆã—ã¦ãŠã„ãŸIAM ãƒ­ãƒ¼ãƒ«ã‚’å–å¾—ã—ã¾ã™ã€‚

```python
import boto3

role_name = "SageMaker-local"

iam = boto3.client("iam")
role = iam.get_role(RoleName=role_name)["Role"]["Arn"]
```

SageMakerã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™ã€‚


```python
import sagemaker as sage

sess = sage.Session()
```

å­¦ç¿’ã«ä½¿ã†ãƒ‡ãƒ¼ã‚¿ã‚’S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
prefix = 'jparacrawl/training'
WORK_DIRECTORY = 'data_train'
data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix)
```

ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒçµ‚äº†ã—ãŸã‚‰ã€SageMakerã®ã‚³ãƒ³ãƒ†ãƒŠã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã€‚

```python
account = sess.boto_session.client('sts').get_caller_identity()['Account']
region = sess.boto_session.region_name
image = '{}.dkr.ecr.{}.amazonaws.com/jparacrawl-train:latest'.format(account, region)

estimator = sage.estimator.Estimator(image,
                       role, 1, 'ml.g4dn.12xlarge',
                       output_path="s3://{}/jparacrawl/output".format(sess.default_bucket()),
                       sagemaker_session=sess)

estimator.fit({"training": data_location})
```

    2022-09-27 12:26:48 Starting - Starting the training job...
    2022-09-27 12:27:11 Starting - Preparing the instances for trainingProfilerReport-1664281607: InProgress
    ......
    2022-09-27 12:28:21 Downloading - Downloading input data.................
    (ä¸­ç•¥)
    [34m[2022-09-27 21:52:40,044 INFO] Validation accuracy: 55.978[0m
    [34m[2022-09-27 21:52:40,087 INFO] Saving checkpoint /opt/ml/model/model.en-ja_step_40000.pt[0m
    
    2022-09-27 21:52:49 Uploading - Uploading generated training model
    2022-09-27 21:58:06 Completed - Training job completed
    Training seconds: 34186
    Billable seconds: 34186


å­¦ç¿’ã«ã¯å¤§ä½“9æ™‚é–“å¼·(33,620 sec)ã‹ã‹ã‚Šã¾ã—ãŸã€‚ãã®åˆ†è²»ç”¨ã‚‚â€¦

å­¦ç¿’çµæœã¯ã€S3ã« `s3://sagemaker-{ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å}-{ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID}/jparacrawl/output/jparacrawl-train-{ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ—¥æ™‚}/output/model.tar.gz` ã®URIã§å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

ä»Šå›å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€[ã“ã¡ã‚‰ã®è¨˜äº‹](https://zenn.dev/thorie/articles/548nl_ctranslate2_setup)ã«ã‚ã‚‹ã‚ˆã†ãªå½¢ã§ç¿»è¨³ã™ã‚‹äº‹ãŒã§ãã¾ã™ã€‚ãŸã ã—ã€ç¿»è¨³ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚


```python:translate.py
#!/usr/bin/env python3

import ctranslate2
import sentencepiece as spm
import sys

translator = ctranslate2.Translator("ctranslate2_model", device="cpu")
sp_en = spm.SentencePieceProcessor("sentencepiece_en.model")
sp_ja = spm.SentencePieceProcessor("sentencepiece_ja.model")

input_text = sys.argv[1]

input_tokens = sp_en.encode(input_text, out_type=str)

results = translator.translate_batch([input_tokens])

output_tokens = results[0].hypotheses[0]
output_text = sp_ja.decode(output_tokens)

print(output_text)
```

ã„ãã¤ã‹ç¿»è¨³ã—ã¦ã¿ãŸçµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚ã‚‚ã†å°‘ã—ã€ã¾ã¨ã‚‚ãªè¨³ãŒå‡ºã‚‹ã‹ã¨æœŸå¾…ã—ã¦ã„ãŸã®ã§ã™ãŒâ€¦ æ®‹å¿µã€‚

| åŸæ–‡ | è¨³æ–‡ |
| ---- | ---- |
| Hello, world! |  â‡ ã€ä¸–ç•Œ! |
| It is fine today. | ä»Šæ—¥ã¯å¤§ä¸ˆå¤«ã§ã™ã€‚ |
| The average viewership in the Kanto region was gauged over an hour from 3 p.m. | ä¹å·åœ°æ–¹ã®å¹³å‡è¦–è´ç‡ã¯ â‡ 3æ™‚ã‹ã‚‰1æ™‚é–“ã«ã‚ãŸã£ã¦ â‡ ã•ã‚Œã¾ã—ãŸã€‚ |
| We are in touch with Hilareeâ€™s family and supporting search and rescue efforts in every way we can. | ãƒ’ãƒ©ãƒªãƒ¼ãƒ•ã‚¡ãƒŸãƒªã«é€£çµ¡ã‚’å–ã‚Šã€ã‚ã‚‰ã‚†ã‚‹æ–¹æ³•ã§æ¤œç´¢ã¨æ•‘ â‡ æ´»å‹•ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ |

